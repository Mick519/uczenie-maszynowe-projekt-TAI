{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Połączenie z google drive"
      ],
      "metadata": {
        "id": "7gvUqvrFbUP4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n3h-by9mbUP5"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Instaowanie rapid"
      ],
      "metadata": {
        "id": "qUOjBT19UeUJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os, sys, io\n",
        "import subprocess\n",
        "from pathlib import Path\n",
        "\n",
        "# Install RAPIDS -- we're doing this in one file, for now, due to ease of use\n",
        "try:\n",
        "  import pynvml\n",
        "except:\n",
        "  output = subprocess.Popen([\"pip install pynvml\"], shell=True, stderr=subprocess.STDOUT,\n",
        "      stdout=subprocess.PIPE)\n",
        "  for line in io.TextIOWrapper(output.stdout, encoding=\"utf-8\"):\n",
        "    if(line == \"\"):\n",
        "      break\n",
        "    else:\n",
        "      print(line.rstrip())\n",
        "  import pynvml\n",
        "try:\n",
        "  pynvml.nvmlInit()\n",
        "except:\n",
        "  raise Exception(\"\"\"\n",
        "                  Unfortunately you're in a Colab instance that doesn't have a GPU.\n",
        "\n",
        "                  Please make sure you've configured Colab to request a GPU Instance Type.\n",
        "\n",
        "                  Go to 'Runtime -> Change Runtime Type --> under the Hardware Accelerator, select GPU', then try again.\"\"\"\n",
        "  )\n",
        "gpu_name = pynvml.nvmlDeviceGetName(pynvml.nvmlDeviceGetHandleByIndex(0))\n",
        "rapids_version = \"24.4.*\"\n",
        "\n",
        "if ('P' not in gpu_name):\n",
        "  print('***********************************************************************')\n",
        "  print('Woo! Your instance has a '+ str(gpu_name)+' GPU!')\n",
        "  print(f'We will install the latest stable RAPIDS via pip {rapids_version}!  Please stand by, should be quick...')\n",
        "  print('***********************************************************************')\n",
        "  print()\n",
        "else:\n",
        "  print('***********************************************************************')\n",
        "  print('Hey! Your instance has a Pascal GPU, a '+ str(gpu_name)+'!')\n",
        "  print('We will install a compatible RAPIDS via pip (23.12)!  Please stand by, should be quick...')\n",
        "  print('***********************************************************************')\n",
        "  print()\n",
        "  rapids_version = \"23.12.*\"\n",
        "\n",
        "\n",
        "output = subprocess.Popen([f\"pip install cudf-cu12=={rapids_version} cuml-cu12=={rapids_version} cugraph-cu12=={rapids_version} cuspatial-cu12=={rapids_version} cuproj-cu12=={rapids_version} cuxfilter-cu12=={rapids_version} cucim-cu12=={rapids_version} pylibraft-cu12=={rapids_version} raft-dask-cu12=={rapids_version} aiohttp --extra-index-url=https://pypi.nvidia.com\"], shell=True, stderr=subprocess.STDOUT,\n",
        "    stdout=subprocess.PIPE)\n",
        "for line in io.TextIOWrapper(output.stdout, encoding=\"utf-8\"):\n",
        "  if(line == \"\"):\n",
        "    break\n",
        "  else:\n",
        "    print(line.rstrip())\n",
        "output = subprocess.Popen([\"rm -rf /usr/local/lib/python3.10/dist-packages/cupy*\"], shell=True, stderr=subprocess.STDOUT,\n",
        "    stdout=subprocess.PIPE)\n",
        "for line in io.TextIOWrapper(output.stdout, encoding=\"utf-8\"):\n",
        "  if(line == \"\"):\n",
        "    break\n",
        "  else:\n",
        "    print(line.rstrip())\n",
        "output = subprocess.Popen([\"pip install cupy-cuda12x\"], shell=True, stderr=subprocess.STDOUT,\n",
        "    stdout=subprocess.PIPE)\n",
        "for line in io.TextIOWrapper(output.stdout, encoding=\"utf-8\"):\n",
        "  if(line == \"\"):\n",
        "    break\n",
        "  else:\n",
        "    print(line.rstrip())\n",
        "print(\"\"\"\n",
        "        ***********************************************************************\n",
        "        The pip install of RAPIDS is complete.\n",
        "\n",
        "        Please do not run any further installation from the conda based installation methods, as they may cause issues!\n",
        "\n",
        "        Please ensure that you're pulling from the git repo to remain updated with the latest working install scripts.\n",
        "\n",
        "        Troubleshooting:\n",
        "            - If there is an installation failure, please check back on RAPIDSAI owned templates/notebooks to see how to update your personal files.\n",
        "            - If an installation failure persists when using the latest script, please make an issue on https://github.com/rapidsai-community/rapidsai-csp-utils\n",
        "        ***********************************************************************\n",
        "        \"\"\"\n",
        "      )"
      ],
      "metadata": {
        "id": "EzwJCBU_TtDK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Niezbędne importy"
      ],
      "metadata": {
        "id": "lQEbJKN8Aau_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import cuml\n",
        "from sklearn.cluster import Birch\n",
        "from cuml.metrics.cluster import silhouette_score\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.metrics import adjusted_rand_score, davies_bouldin_score\n",
        "from scipy.spatial.distance import cdist"
      ],
      "metadata": {
        "id": "dv8ZzghCiTCC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Wczytanie danych"
      ],
      "metadata": {
        "id": "fJdriab5Nd_5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "embeddings_path = '/content/drive/My Drive/data_sety/normalized_fasttext_embeddings.csv'\n",
        "data_embeddings = pd.read_csv(embeddings_path)\n",
        "\n",
        "original_data_path = '/content/drive/My Drive/data_sety/ready_data_set.csv'\n",
        "original_data = pd.read_csv(original_data_path)\n",
        "labels_true = original_data['label']\n"
      ],
      "metadata": {
        "id": "8AWu8X1r-o0g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Funkcja licząca indeks dunn"
      ],
      "metadata": {
        "id": "dKw78W83UzMr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def dunn_index(X, labels):\n",
        "    distances = cdist(X, X, 'euclidean') # Obliczenie odleŋłości między wszystkimi punktami i zapisanie ich w macierzy\n",
        "    unique_clusters = np.unique(labels) # Identyfikacja unikalnych klastrów.\n",
        "\n",
        "    # Inicjalizacja list dla odległości między klastrami i średnic klastrów\n",
        "    inter_cluster_distances = []\n",
        "    intra_cluster_diameters = []\n",
        "\n",
        "    # Obliczanie odległości między klastrami i średnic klastrów\n",
        "    for i in unique_clusters:\n",
        "        for j in unique_clusters:\n",
        "            if i != j:\n",
        "                inter_cluster_distances.append(np.min(distances[labels == i][:, labels == j]))\n",
        "        intra_cluster_diameters.append(np.max(distances[labels == i][:, labels == i]))\n",
        "\n",
        "    # Zwrócenie indeksu Dunn. Jest on stosunkiem najmniejszej odległości między klastrami do największej średnicy klastra.\n",
        "    return np.min(inter_cluster_distances) / np.max(intra_cluster_diameters)"
      ],
      "metadata": {
        "id": "3lHg8QmO-zgB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Przygotowanie df do zbierania wyników i zklastrowanych danych"
      ],
      "metadata": {
        "id": "tDHUVQcGBG4m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "results_df = pd.DataFrame()\n",
        "results_list = []"
      ],
      "metadata": {
        "id": "mwPXsrhVg9Da"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "silhouette_scores_test = []\n",
        "db_scores_test = []\n",
        "ari_scores_test = []\n",
        "dunn_scores_test = []"
      ],
      "metadata": {
        "id": "Ke5r0GJ7g9Db"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "silhouette_scores_train = []\n",
        "db_scores_train = []\n",
        "ari_scores_train = []\n",
        "dunn_scores_train = []"
      ],
      "metadata": {
        "id": "wAW2rYdCJfaA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Klasteryzacja z wykorzystaniem 5-krotnej walidacji krzyżowej"
      ],
      "metadata": {
        "id": "BaBbBn94AptU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_embeddings_array = data_embeddings.to_numpy()\n",
        "\n",
        "\n",
        "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "fold_number = 0\n",
        "for train_index, test_index in kf.split(data_embeddings_array):\n",
        "    fold_number += 1\n",
        "    X_train, X_test = data_embeddings.iloc[train_index], data_embeddings.iloc[test_index]\n",
        "    y_train, y_test = labels_true[train_index], labels_true[test_index]\n",
        "\n",
        "    # Klasteryzacja za pomocą K-means\n",
        "    birch = Birch(n_clusters=2) # Inicjalizacja modelu klasteryzatora\n",
        "\n",
        "    labels_pred_train = birch.fit_predict(X_train) # Trenowanie oraz przypisywanie danych treningowych do jednego z dwóch klastrów\n",
        "    labels_pred_test = birch.fit_predict(X_test) # Trenowanie oraz przypisywanie danych testowych do jednego z dwóch klastrów\n",
        "\n",
        "    # Obliczanie metryk dla danych testowych\n",
        "    silhouette_test = silhouette_score(X_test, labels_pred_test)\n",
        "    db_index_test = davies_bouldin_score(X_test, labels_pred_test)\n",
        "    ari_test = adjusted_rand_score(y_test, labels_pred_test)\n",
        "    dunn_test = dunn_index(X_test, labels_pred_test)\n",
        "\n",
        "    # Obliczanie metryk dla danych treningowych\n",
        "    silhouette_train = silhouette_score(X_train, labels_pred_train)\n",
        "    db_index_train = davies_bouldin_score(X_train, labels_pred_train)\n",
        "    ari_train = adjusted_rand_score(y_train, labels_pred_train)\n",
        "    dunn_train = dunn_index(X_train, labels_pred_train)\n",
        "\n",
        "    # Dodanie wyników do listy wyników w celu obliczenia średniej na koniec dla danych testowych\n",
        "    silhouette_scores_test.append(silhouette_test)\n",
        "    db_scores_test.append(db_index_test)\n",
        "    ari_scores_test.append(ari_test)\n",
        "    dunn_scores_test.append(dunn_test)\n",
        "\n",
        "    # Dodanie wyników do listy wyników w celu obliczenia średniej na koniec dla danych treningowych\n",
        "    silhouette_scores_train.append(silhouette_train)\n",
        "    db_scores_train.append(db_index_train)\n",
        "    ari_scores_train.append(ari_train)\n",
        "    dunn_scores_train.append(dunn_train)\n",
        "\n",
        "    results_list.append({\n",
        "        'Fold': fold_number,\n",
        "        'Silhouette Score Test': silhouette_test,\n",
        "        'DB score Test': db_index_test,\n",
        "        'ARI Score Test': ari_test,\n",
        "        'Dunn Index Test': dunn_test,\n",
        "        'Silhouette Score Train': silhouette_train,\n",
        "        'DB score Train': db_index_train,\n",
        "        'ARI Score Train': ari_train,\n",
        "        'Dunn Index Train': dunn_train\n",
        "    })\n",
        "\n",
        "    # Wyświetlenie wyników częściowych dla danego etapu\n",
        "    print(\"Fold \" + str(fold_number) + \" Silhouette Score Test: \" + str(silhouette_test) + \" DB Score Test: \" + str(db_index_test) + \" ARI Score Test: \" + str(ari_test) + \" Dunn Index Test: \" + str(dunn_test) + \" Silhouette Score Train: \" + str(silhouette_train) +  \" DB Score Train: \" + str(db_index_train) + \" ARI Score Train: \" + str(ari_train) + \" Dunn Index Train: \" + str(dunn_train))"
      ],
      "metadata": {
        "id": "uxJP7yB5a2Ls"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Wyświeylenie podsumowania"
      ],
      "metadata": {
        "id": "WBYBOejcU1g9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "average_silhouette_test = np.mean(silhouette_scores_test)\n",
        "average_db_test = np.mean(db_scores_test)\n",
        "average_ari_test = np.mean(ari_scores_test)\n",
        "average_dunn_test = np.mean(dunn_scores_test)\n",
        "\n",
        "average_silhouette_train = np.mean(silhouette_scores_train)\n",
        "average_db_train = np.mean(db_scores_train)\n",
        "average_ari_train = np.mean(ari_scores_train)\n",
        "average_dunn_train = np.mean(dunn_scores_train)\n",
        "\n",
        "print(\"Average Silhouette Score Test:\", average_silhouette_test)\n",
        "print(\"Average Davies-Bouldin Score Test:\", average_db_test)\n",
        "print(\"Average Adjusted Rand Index Test:\", average_ari_test)\n",
        "print(\"Average Dunn Index Test:\", average_dunn_test)\n",
        "\n",
        "print(\"Average Silhouette Score Train:\", average_silhouette_train)\n",
        "print(\"Average Davies-Bouldin Score Train:\", average_db_train)\n",
        "print(\"Average Adjusted Rand Index Train:\", average_ari_train)\n",
        "print(\"Average Dunn Index Train:\", average_dunn_train)"
      ],
      "metadata": {
        "id": "SJCJdswY-5oW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Zapisanie wyników"
      ],
      "metadata": {
        "id": "THIOLpngBqoV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "results_file_path = '/content/drive/My Drive/data_sety/fasttext_birch_kfold_clustering_metrics.csv'\n",
        "\n",
        "results_df = pd.DataFrame(results_list)\n",
        "results_df.to_csv(results_file_path, index=False)"
      ],
      "metadata": {
        "id": "iaTS_qKkBsgL"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}