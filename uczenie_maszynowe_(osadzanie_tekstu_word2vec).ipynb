{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Wczytanie zbioru danych"
      ],
      "metadata": {
        "id": "L-SINQyfkiGA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KmrczEARkhz9",
        "outputId": "1d12a478-010d-448b-d2c0-91b0b558a992"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pircIVI1kBXX",
        "outputId": "62d4ca0a-61d0-45f1-b8e5-20f20bf79e21"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   label                                               text  \\\n",
            "0      1  ounce feather bowl hummingbird opec moment ala...   \n",
            "1      1  wulvob get your medircations online qnb ikud v...   \n",
            "2      0   computer connection from cnn com wednesday es...   \n",
            "3      1  university degree obtain a prosperous future m...   \n",
            "4      0  thanks for all your answers guys i know i shou...   \n",
            "\n",
            "                                       filtered_text  \\\n",
            "0  ounce feather bowl hummingbird opec moment ala...   \n",
            "1  wulvob get your medircations online qnb ikud v...   \n",
            "2   computer connection from cnn com wednesday es...   \n",
            "3  university degree obtain a prosperous future m...   \n",
            "4  thanks for all your answers guys i know i shou...   \n",
            "\n",
            "                                     normalized_text  \\\n",
            "0  ounce feather bowl hummingbird opec moment ala...   \n",
            "1  wulvob get your medircations online qnb ikud v...   \n",
            "2   computer connection from cnn com wednesday es...   \n",
            "3  university degree obtain a prosperous future m...   \n",
            "4  thanks for all your answers guys i know i shou...   \n",
            "\n",
            "                                      tokenized_text  \\\n",
            "0  ['ounce', 'feather', 'bowl', 'hummingbird', 'o...   \n",
            "1  ['wulvob', 'get', 'your', 'medircations', 'onl...   \n",
            "2  ['computer', 'connection', 'from', 'cnn', 'com...   \n",
            "3  ['university', 'degree', 'obtain', 'a', 'prosp...   \n",
            "4  ['thanks', 'for', 'all', 'your', 'answers', 'g...   \n",
            "\n",
            "                                     filtered_tokens  \\\n",
            "0  ['ounce', 'feather', 'bowl', 'hummingbird', 'o...   \n",
            "1  ['wulvob', 'get', 'medircations', 'online', 'q...   \n",
            "2  ['computer', 'connection', 'cnn', 'com', 'wedn...   \n",
            "3  ['university', 'degree', 'obtain', 'prosperous...   \n",
            "4  ['thanks', 'answers', 'guys', 'know', 'checked...   \n",
            "\n",
            "                                     lemmatized_text  \n",
            "0  ['ounce', 'feather', 'bowl', 'hummingbird', 'o...  \n",
            "1  ['wulvob', 'get', 'medircation', 'online', 'qn...  \n",
            "2  ['computer', 'connection', 'cnn', 'com', 'wedn...  \n",
            "3  ['university', 'degree', 'obtain', 'prosperous...  \n",
            "4  ['thank', 'answer', 'guy', 'know', 'check', 'r...  \n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "file_path = '/content/drive/My Drive/data_sety/ready_data_set.csv'\n",
        "data_set = pd.read_csv(file_path)\n",
        "print(data_set.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Zamiana ciągów znaków na listę słów"
      ],
      "metadata": {
        "id": "Nc43QdAdHbQ9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models import Word2Vec\n",
        "\n",
        "import ast\n",
        "data_set['lemmatized_text'] = data_set['lemmatized_text'].apply(ast.literal_eval)"
      ],
      "metadata": {
        "id": "l2VSCBmVHb89"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(data_set.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lx91O_EUKBhf",
        "outputId": "e27fe779-affd-42bd-b3e7-7cbe6f0513e7"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   label                                               text  \\\n",
            "0      1  ounce feather bowl hummingbird opec moment ala...   \n",
            "1      1  wulvob get your medircations online qnb ikud v...   \n",
            "2      0   computer connection from cnn com wednesday es...   \n",
            "3      1  university degree obtain a prosperous future m...   \n",
            "4      0  thanks for all your answers guys i know i shou...   \n",
            "\n",
            "                                       filtered_text  \\\n",
            "0  ounce feather bowl hummingbird opec moment ala...   \n",
            "1  wulvob get your medircations online qnb ikud v...   \n",
            "2   computer connection from cnn com wednesday es...   \n",
            "3  university degree obtain a prosperous future m...   \n",
            "4  thanks for all your answers guys i know i shou...   \n",
            "\n",
            "                                     normalized_text  \\\n",
            "0  ounce feather bowl hummingbird opec moment ala...   \n",
            "1  wulvob get your medircations online qnb ikud v...   \n",
            "2   computer connection from cnn com wednesday es...   \n",
            "3  university degree obtain a prosperous future m...   \n",
            "4  thanks for all your answers guys i know i shou...   \n",
            "\n",
            "                                      tokenized_text  \\\n",
            "0  ['ounce', 'feather', 'bowl', 'hummingbird', 'o...   \n",
            "1  ['wulvob', 'get', 'your', 'medircations', 'onl...   \n",
            "2  ['computer', 'connection', 'from', 'cnn', 'com...   \n",
            "3  ['university', 'degree', 'obtain', 'a', 'prosp...   \n",
            "4  ['thanks', 'for', 'all', 'your', 'answers', 'g...   \n",
            "\n",
            "                                     filtered_tokens  \\\n",
            "0  ['ounce', 'feather', 'bowl', 'hummingbird', 'o...   \n",
            "1  ['wulvob', 'get', 'medircations', 'online', 'q...   \n",
            "2  ['computer', 'connection', 'cnn', 'com', 'wedn...   \n",
            "3  ['university', 'degree', 'obtain', 'prosperous...   \n",
            "4  ['thanks', 'answers', 'guys', 'know', 'checked...   \n",
            "\n",
            "                                     lemmatized_text  \n",
            "0  [ounce, feather, bowl, hummingbird, opec, mome...  \n",
            "1  [wulvob, get, medircation, online, qnb, ikud, ...  \n",
            "2  [computer, connection, cnn, com, wednesday, es...  \n",
            "3  [university, degree, obtain, prosperous, futur...  \n",
            "4  [thank, answer, guy, know, check, rsync, manua...  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Trenowanie modelu word2vec. sentences - dane do treningu, vector_size - rozmiar wektora cech (wymiarów) dla każdego słowa, window - maksymalna odległość między aktualnym słowem a przewidywanym w kontekście, min_count - minimalna liczba wystąpień słowa w dokumencie, aby było ono uwzględnione w treningu, workers - liczba wątków roboczych do trenowania modelu"
      ],
      "metadata": {
        "id": "wda8KsQ4JhB2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = Word2Vec(sentences=data_set['lemmatized_text'], vector_size=100, window=5, min_count=1, workers=4)"
      ],
      "metadata": {
        "id": "2zw0THWtJjiu"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Zapisanie modelu w folderze"
      ],
      "metadata": {
        "id": "Dkae2JZON9YO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models import Word2Vec\n",
        "\n",
        "model_path = '/content/drive/My Drive/data_sety/word_models/word2vec_model.model'\n",
        "model.save(model_path)"
      ],
      "metadata": {
        "id": "12CSHjsiN74M"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Osadzanie tekstów w całym zbiorze danych"
      ],
      "metadata": {
        "id": "WRklA-mzOXei"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def document_word2vec_embedding(doc, model):\n",
        "    vectors = [model.wv[word] for word in doc if word in model.wv]\n",
        "\n",
        "    if len(vectors) == 0:\n",
        "        return np.zeros(model.vector_size)\n",
        "\n",
        "    return np.mean(vectors, axis=0)\n",
        "\n",
        "word2vec_embeddings = np.array([document_word2vec_embedding(doc, model) for doc in data_set['lemmatized_text']])\n"
      ],
      "metadata": {
        "id": "2UEVlK0OOfr7"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Normalizacja wektorów osadzeń do normy euklidesowej - Dzięki temu różnice w długości wektorów nie wpłyną na wyniki algorytmów klasteryzacji\n",
        "\n",
        "Po normalizacji, punkty są skalowane w taki sposób, by ich odległość od początku układu współrzędnych wynosiła 1."
      ],
      "metadata": {
        "id": "tVnxp7PNUmEr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import normalize\n",
        "\n",
        "normalized_embeddings = normalize(word2vec_embeddings, norm='l2', axis=1)"
      ],
      "metadata": {
        "id": "SvDPCpnVUuNg"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Usuwanie outlierów - obserwacji, które znacząco odbiegają od pozostałych danych i mogą wpływać na wyniki klasteryzacji"
      ],
      "metadata": {
        "id": "3hiC04PXW7I5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy import stats\n",
        "\n",
        "word2vec_embeddings_df = pd.DataFrame(normalized_embeddings)\n",
        "\n",
        "z_scores = np.abs(stats.zscore(word2vec_embeddings_df))\n",
        "\n",
        "threshold = 3\n",
        "\n",
        "word2vec_embeddings_df = word2vec_embeddings_df[(z_scores < threshold).all(axis=1)]"
      ],
      "metadata": {
        "id": "D-SS57wTYsLh"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Zapisanie zbioru gotowego do klasteryzacji"
      ],
      "metadata": {
        "id": "vi2-rhM9aXgf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "word2vec_embeddings_df.to_csv('/content/drive/My Drive/data_sety/normalized_filtered_word2vec_embeddings.csv', index=False)"
      ],
      "metadata": {
        "id": "fkFSPyp6ZiIU"
      },
      "execution_count": 11,
      "outputs": []
    }
  ]
}